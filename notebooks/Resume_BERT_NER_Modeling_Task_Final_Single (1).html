
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Hackathon Modeling Task – BERT Resume NER (Complete HTML)</title>
<style>
body { font-family: Arial, Helvetica, sans-serif; margin: 48px; line-height: 1.8; }
h1 { color:#0d47a1; }
h2 { color:#1565c0; margin-top:40px; }
h3 { color:#1e88e5; }
pre { background:#f5f7fa; padding:18px; border-radius:6px; overflow-x:auto; }
code { color:#b71c1c; }
ul { margin-left:22px; }
.section { margin-bottom:40px; }
.footer { margin-top:60px; font-size:14px; color:#555; }
</style>
</head>
<body>

<h1>Hackathon Modeling Task Documentation</h1>
<h2>BERT Fine-Tuning for Resume Named Entity Recognition (NER)</h2>

<div class="section">
<h2>1. Objective</h2>
<p>
The objective of this modeling task is to automatically extract structured information from
unstructured resume text using <strong>Named Entity Recognition (NER)</strong>. The system identifies
key entities such as skills, education, experience, and certifications to support downstream
applications like candidate profiling and matching.
</p>
</div>

<div class="section">
<h2>2. Architecture Implemented</h2>
<pre>
Resume Text
   ↓
BERT Tokenizer (Subword Tokenization)
   ↓
BERT Encoder (Pre-trained & Fine-tuned)
   ↓
Token Classification Head
   ↓
BIO Entity Labels
   ↓
Structured Resume JSON
</pre>
<p>
A transformer-based BERT model is fine-tuned for token-level classification. Each token is
assigned an entity label using the BIO tagging scheme.
</p>
</div>

<div class="section">
<h2>3. Entity Schema</h2>
<ul>
<li><strong>SKILL</strong> – Technical and professional skills</li>
<li><strong>EDUCATION</strong> – Degrees and institutions</li>
<li><strong>EXPERIENCE</strong> – Years and job roles</li>
<li><strong>CERTIFICATION</strong> – Professional certifications</li>
</ul>
</div>

<div class="section">
<h2>4. Complete Modeling Code</h2>
<pre><code>
from datasets import load_dataset
from transformers import (
    BertTokenizerFast,
    BertForTokenClassification,
    Trainer,
    TrainingArguments,
    DataCollatorForTokenClassification
)
import torch, json, os

label_list = [
    "O",
    "B-SKILL", "I-SKILL",
    "B-EDU", "I-EDU",
    "B-EXP", "I-EXP",
    "B-CERT", "I-CERT"
]

label2id = {l: i for i, l in enumerate(label_list)}   
id2label = {i: l for l, i in label2id.items()}

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

def tokenize_and_align_labels(example):
    tokenized = tokenizer(example["tokens"], is_split_into_words=True, truncation=True)
    labels = []
    for word_id in tokenized.word_ids():
        labels.append(-100 if word_id is None else label2id[example["ner_tags"][word_id]])
    tokenized["labels"] = labels
    return tokenized

model = BertForTokenClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=len(label_list),
    id2label=id2label,
    label2id=label2id
)

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    data_collator=DataCollatorForTokenClassification(tokenizer)
)

trainer.train()
</code></pre>
</div>

<div class="section">
<h2>5. Inference Example</h2>
<pre>
Resume Input:
I have 5 years of Software Engineering experience.
Skilled in Python and Docker.
Completed MS Data Science from EPITA.
AWS Solutions Architect certified.
</pre>

<h3>Model Output</h3>
<pre>
{
  "skills": ["Python", "Docker"],
  "education": "MS Data Science, EPITA",
  "experience": "5 years Software Engineering",
  "certifications": ["AWS Solutions Architect"]
}
</pre>
</div>

<div class="section">
<h2>6. Planned Changes & Future Work</h2>
<ul>
<li>Train on larger and real-world annotated resume datasets</li>
<li>Apply LoRA for parameter-efficient fine-tuning</li>
<li>Add CRF decoding for improved label consistency</li>
<li>Support long and multi-page resumes</li>
<li>Add confidence scores for extracted entities</li>
<li>Enable multi-language resume processing</li>
</ul>
</div>

<div class="section">
<h2>7. Interaction with the MVP</h2>
<pre>
Resume Upload
   ↓
Text Extraction Service
   ↓
NER Modeling Service (This Component)
   ↓
Structured Candidate Profile
   ↓
Search, Ranking, and Matching Engine
</pre>
<p>
Within the MVP, this model operates as a backend AI service. It converts unstructured resume
text into structured candidate profiles used by other components for search and analytics.
</p>
</div>

<div class="section">
<h2>8. Conclusion</h2>
<p>
This modeling task demonstrates a complete NLP pipeline using BERT fine-tuning for resume
NER. The solution is modular, scalable, and forms a core intelligence component of the MVP.
</p>
</div>

<div class="footer">
<strong>Hackathon Modeling Task – Single HTML File</strong>
</div>

</body>
</html>
