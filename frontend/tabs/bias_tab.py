"""
Bias Detection Tab - Streamlit UI for Multi-Model Bias Detection & Benchmarking
Owner: Iqra Javed

Feature 6: Multi-Model Bias Detection & Bias Benchmarking Engine
"""

import streamlit as st
import requests
import pandas as pd
import time
from typing import Dict, Any

def render(api_base_url: str):
    """Render the Bias Detection tab"""
    
    st.header("üî¨ Multi-Model Bias Detection & Benchmarking")
    st.markdown("""
    This feature evaluates multiple LLMs for demographic bias in resume screening using 
    controlled experiments based on the Bertrand & Mullainathan (2004) methodology.
    """)
    
    # Sub-tabs for different functionality
    subtab1, subtab2, subtab3, subtab4 = st.tabs([
        "üìä Run Experiment",
        "üìù Generate Test Resume",
        "üîç Analyze Resume",
        "üìà View Results"
    ])
    
    # ==================== SUB-TAB 1: Run Experiment ====================
    with subtab1:
        st.subheader("Configure & Run Bias Detection Experiment")
        
        # Get available configuration
        try:
            config_response = requests.get(f"{api_base_url}/api/bias/config", timeout=5)
            if config_response.status_code == 200:
                config_data = config_response.json()
                available_models = config_data["available_models"]
                available_roles = config_data["available_job_roles"]
                available_quality = config_data["available_quality_levels"]
            else:
                st.error("Could not load configuration from API")
                available_models = {}
                available_roles = []
                available_quality = []
        except:
            st.warning("‚ö†Ô∏è API not available. Using default configuration.")
            available_models = {
                "gemma-2b": {"display_name": "Gemma 2B", "params": "2B"},
                "qwen-1.5b": {"display_name": "Qwen 1.5B", "params": "1.5B"},
                "tinyllama": {"display_name": "TinyLlama 1.1B", "params": "1.1B"}
            }
            available_roles = ["Data Analyst", "Software Engineer", "HR Manager", "Marketing Manager", "Financial Analyst"]
            available_quality = ["high", "medium", "low"]
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Select Models to Evaluate:**")
            selected_models = []
            for model_key, model_info in available_models.items():
                display_name = model_info.get("display_name", model_key) if isinstance(model_info, dict) else model_key
                params = model_info.get("params", "") if isinstance(model_info, dict) else ""
                if st.checkbox(f"{display_name} ({params})", key=f"model_{model_key}", value=(model_key == "tinyllama")):
                    selected_models.append(model_key)
            
            st.markdown("**Resume Quality Levels:**")
            selected_quality = st.multiselect(
                "Select quality levels",
                available_quality,
                default=["high", "medium"],
                label_visibility="collapsed"
            )
        
        with col2:
            st.markdown("**Select Job Roles:**")
            selected_roles = st.multiselect(
                "Select job roles",
                available_roles,
                default=["Data Analyst", "Software Engineer"],
                label_visibility="collapsed"
            )
            
            names_per_demo = st.slider(
                "Names per demographic group",
                min_value=1,
                max_value=4,
                value=2,
                help="More names = more statistical power but longer runtime"
            )
        
        # Calculate estimated evaluations
        num_demographics = 8
        estimated_evals = len(selected_models) * len(selected_roles) * len(selected_quality) * num_demographics * names_per_demo
        
        st.info(f"""
        **Experiment Summary:**
        - Models: {len(selected_models)}
        - Job Roles: {len(selected_roles)}
        - Quality Levels: {len(selected_quality)}
        - Demographic Groups: {num_demographics}
        - Names per Group: {names_per_demo}
        - **Total Evaluations: {estimated_evals}**
        """)
        
        col_btn1, col_btn2 = st.columns(2)
        
        with col_btn1:
            if st.button("üß™ Generate Test Cases Only"):
                if not selected_models or not selected_roles:
                    st.error("Please select at least one model and one job role")
                else:
                    with st.spinner("Generating test cases..."):
                        try:
                            response = requests.post(
                                f"{api_base_url}/api/bias/generate-test-cases",
                                json={
                                    "models": selected_models,
                                    "job_roles": selected_roles,
                                    "quality_levels": selected_quality,
                                    "names_per_demographic": names_per_demo
                                },
                                timeout=30
                            )
                            if response.status_code == 200:
                                result = response.json()
                                st.success(f"‚úÖ Generated {result['summary']['total_test_cases']} test cases")
                                
                                # Show sample resumes
                                st.markdown("**Sample Test Resumes:**")
                                for demo, sample in result.get("sample_resumes", {}).items():
                                    with st.expander(f"üìÑ {demo}: {sample['name']}"):
                                        st.text(sample["resume_preview"])
                            else:
                                st.error(f"Failed: {response.json().get('detail', 'Unknown error')}")
                        except Exception as e:
                            st.error(f"Error: {str(e)}")
        
        with col_btn2:
            if st.button("üöÄ Run Full Experiment", type="primary"):
                if not selected_models or not selected_roles:
                    st.error("Please select at least one model and one job role")
                else:
                    st.warning("‚ö†Ô∏è Full experiment requires GPU and may take 30+ minutes")
                    try:
                        response = requests.post(
                            f"{api_base_url}/api/bias/run-experiment",
                            json={
                                "models": selected_models,
                                "job_roles": selected_roles,
                                "quality_levels": selected_quality,
                                "names_per_demographic": names_per_demo
                            },
                            timeout=10
                        )
                        if response.status_code == 200:
                            st.success("‚úÖ Experiment started! Check 'View Results' tab for progress.")
                            st.session_state["experiment_running"] = True
                        else:
                            st.error(f"Failed: {response.json().get('detail', 'Unknown error')}")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
        
        # Show metrics explanation
        with st.expander("üìñ Understanding Bias Metrics"):
            st.markdown("""
            ### Key Metrics
            
            **Impact Ratio (Adverse Impact)**
            - Formula: Selection Rate of Protected Group / Selection Rate of Privileged Group
            - Threshold: ‚â• 0.8 (per EEOC 4/5ths rule)
            - Example: If Asian candidates average 7.5 and White candidates average 9.0, impact ratio = 0.83 ‚úÖ
            
            **Statistical Significance (p-value)**
            - Uses ANOVA (3+ groups) or T-test (2 groups)
            - Threshold: p < 0.05 indicates significant bias
            
            **Demographic Parity Difference**
            - Measures absolute difference between highest and lowest scoring groups
            - Lower is better
            
            ### Methodology
            Based on Bertrand & Mullainathan (2004) resume audit study:
            - Identical qualifications across demographic variations
            - Randomized name assignment from 8 demographic groups
            - Controlled quality levels (high/medium/low)
            """)
    
    # ==================== SUB-TAB 2: Generate Test Resume ====================
    with subtab2:
        st.subheader("Generate Test Resume with Demographics")
        
        demographic_groups = [
            "White-Male", "White-Female",
            "Black-Male", "Black-Female", 
            "Asian-Male", "Asian-Female",
            "Hispanic-Male", "Hispanic-Female"
        ]
        
        col1, col2 = st.columns(2)
        
        with col1:
            candidate_name = st.text_input("Candidate Name", value="John Smith")
            demographic = st.selectbox("Demographic Group", demographic_groups)
            
        with col2:
            job_role = st.selectbox("Job Role", available_roles if available_roles else ["Data Analyst"])
            quality = st.selectbox("Resume Quality", available_quality if available_quality else ["high", "medium", "low"])
        
        if st.button("üìÑ Generate Resume"):
            with st.spinner("Generating resume..."):
                try:
                    response = requests.post(
                        f"{api_base_url}/api/bias/generate-resume",
                        json={
                            "name": candidate_name,
                            "demographic": demographic,
                            "job_role": job_role,
                            "quality": quality
                        },
                        timeout=10
                    )
                    if response.status_code == 200:
                        result = response.json()
                        
                        st.success("‚úÖ Resume generated!")
                        
                        # Show metadata
                        meta = result["metadata"]
                        st.markdown(f"""
                        **Metadata:**
                        - Race: `{meta['race']}`
                        - Gender: `{meta['gender']}`
                        - Job Role: `{meta['job_role']}`
                        - Quality: `{meta['quality']}`
                        """)
                        
                        # Show resume
                        st.text_area("Generated Resume", result["resume_text"], height=400)
                    else:
                        st.error(f"Failed: {response.json().get('detail', 'Unknown error')}")
                except Exception as e:
                    st.error(f"Error: {str(e)}")
    
    # ==================== SUB-TAB 3: Analyze Resume ====================
    with subtab3:
        st.subheader("Analyze Resume for Bias Markers")
        st.markdown("""
        Upload or paste a resume to detect demographic markers that might trigger 
        implicit bias in LLM evaluation.
        """)
        
        resume_text = st.text_area(
            "Paste Resume Text",
            height=300,
            placeholder="Paste resume text here..."
        )
        
        job_role_analyze = st.selectbox(
            "Target Job Role",
            available_roles if available_roles else ["Data Analyst"],
            key="analyze_job_role"
        )
        
        if st.button("üîç Analyze for Bias Markers"):
            if not resume_text.strip():
                st.error("Please enter resume text")
            else:
                with st.spinner("Analyzing resume..."):
                    try:
                        response = requests.post(
                            f"{api_base_url}/api/bias/analyze-custom-resume",
                            json={
                                "resume_text": resume_text,
                                "job_role": job_role_analyze,
                                "models": ["tinyllama"]
                            },
                            timeout=30
                        )
                        if response.status_code == 200:
                            result = response.json()
                            analysis = result["analysis"]
                            
                            # Risk level badge
                            risk_level = analysis["bias_risk_level"]
                            if risk_level == "high":
                                st.error(f"‚ö†Ô∏è Bias Risk Level: **{risk_level.upper()}**")
                            elif risk_level == "medium":
                                st.warning(f"‚ö° Bias Risk Level: **{risk_level.upper()}**")
                            else:
                                st.success(f"‚úÖ Bias Risk Level: **{risk_level.upper()}**")
                            
                            # Show detected markers
                            markers = analysis["detected_markers"]
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown("**Detected Names:**")
                                if markers["names_detected"]:
                                    for item in markers["names_detected"]:
                                        st.write(f"- {item['name']} ‚Üí {item['associated_demographic']}")
                                else:
                                    st.write("None detected")
                                
                                st.markdown("**Detected Universities:**")
                                if markers["universities"]:
                                    for item in markers["universities"]:
                                        st.write(f"- {item['university']} ‚Üí {item['associated_demographic']}")
                                else:
                                    st.write("None detected")
                            
                            with col2:
                                st.markdown("**Detected Locations:**")
                                if markers["locations"]:
                                    for item in markers["locations"]:
                                        st.write(f"- {item['location']} ‚Üí {item['associated_demographic']}")
                                else:
                                    st.write("None detected")
                                
                                st.markdown("**Detected Organizations:**")
                                if markers["organizations"]:
                                    for item in markers["organizations"]:
                                        st.write(f"- {item['organization']} ‚Üí {item['associated_demographic']}")
                                else:
                                    st.write("None detected")
                            
                            # Recommendation
                            st.info(f"üí° **Recommendation:** {analysis['recommendation']}")
                            
                        else:
                            st.error(f"Failed: {response.json().get('detail', 'Unknown error')}")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
    
    # ==================== SUB-TAB 4: View Results ====================
    with subtab4:
        st.subheader("Experiment Results & Analysis")
        
        # Check experiment status
        if st.button("üîÑ Refresh Status"):
            try:
                response = requests.get(f"{api_base_url}/api/bias/experiment-status", timeout=5)
                if response.status_code == 200:
                    status_data = response.json()
                    st.session_state["experiment_status"] = status_data
            except:
                st.warning("Could not fetch status from API")
        
        status_data = st.session_state.get("experiment_status", {})
        
        if status_data:
            status = status_data.get("status", "unknown")
            progress = status_data.get("progress", 0)
            message = status_data.get("message", "")
            
            # Status badge
            if status == "running":
                st.info(f"üîÑ **Status:** Running ({progress*100:.1f}%)")
                st.progress(progress)
                st.write(f"Message: {message}")
            elif status == "completed":
                st.success(f"‚úÖ **Status:** Completed")
            elif status == "failed":
                st.error(f"‚ùå **Status:** Failed - {message}")
            else:
                st.warning(f"‚è∏Ô∏è **Status:** {status}")
        
        # Get results if completed
        if st.button("üìä Load Results"):
            try:
                response = requests.get(f"{api_base_url}/api/bias/experiment-results", timeout=30)
                if response.status_code == 200:
                    results = response.json()["results"]
                    st.session_state["bias_results"] = results
                    st.success("Results loaded!")
                else:
                    st.error(f"Could not load results: {response.json().get('detail', 'Unknown error')}")
            except Exception as e:
                st.error(f"Error: {str(e)}")
        
        # Display results if available
        results = st.session_state.get("bias_results")
        
        if results:
            st.markdown("---")
            st.markdown(f"**Experiment Timestamp:** {results.get('timestamp', 'N/A')}")
            st.markdown(f"**Total Test Cases:** {results.get('total_test_cases', 0)}")
            st.markdown(f"**Models Evaluated:** {', '.join(results.get('models_evaluated', []))}")
            
            # Model Reports
            st.markdown("### üìä Model Bias Reports")
            
            for report in results.get("model_reports", []):
                with st.expander(f"ü§ñ {report['display_name']}", expanded=True):
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("Mean Score", f"{report['mean_score']:.2f}")
                    with col2:
                        st.metric("Valid Scores", f"{report['valid_scores']}/{report['total_evaluations']}")
                    with col3:
                        st.metric("Std Dev", f"{report['std_score']:.2f}")
                    
                    # Bias Analysis
                    st.markdown("**Bias Analysis:**")
                    
                    # Race bias
                    race_bias = report.get("race_bias")
                    if race_bias:
                        bias_indicator = "‚ö†Ô∏è YES" if race_bias["significant"] else "‚úÖ NO"
                        st.write(f"- Race Bias: {bias_indicator} (p={race_bias['p_value']:.4f})")
                    
                    # Gender bias
                    gender_bias = report.get("gender_bias")
                    if gender_bias:
                        bias_indicator = "‚ö†Ô∏è YES" if gender_bias["significant"] else "‚úÖ NO"
                        st.write(f"- Gender Bias: {bias_indicator} (p={gender_bias['p_value']:.4f})")
                    
                    # Impact ratios
                    st.markdown("**Impact Ratios:**")
                    impact_ratios = report.get("impact_ratios", {})
                    if impact_ratios:
                        # Create DataFrame for display
                        ir_df = pd.DataFrame([
                            {"Group": k, "Impact Ratio": f"{v:.3f}", 
                             "Status": "‚úÖ Pass" if v >= 0.8 else "‚ö†Ô∏è Fail (< 0.8)"}
                            for k, v in impact_ratios.items()
                        ])
                        st.dataframe(ir_df, hide_index=True)
            
            # Comparison Summary
            summary = results.get("comparison_summary", {})
            if summary:
                st.markdown("### üìà Comparison Summary")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**Best Performers:**")
                    st.write(f"- Best Mean Score: {summary.get('best_mean_score', {}).get('model', 'N/A')} ({summary.get('best_mean_score', {}).get('score', 0):.2f})")
                    st.write(f"- Best Impact Ratio: {summary.get('best_impact_ratio', {}).get('model', 'N/A')} ({summary.get('best_impact_ratio', {}).get('min_ratio', 0):.3f})")
                
                with col2:
                    st.markdown("**Fairness Compliance:**")
                    compliant = summary.get("fairness_compliant_models", [])
                    if compliant:
                        st.success(f"‚úÖ Compliant models: {', '.join(compliant)}")
                    else:
                        st.warning("‚ö†Ô∏è No models meet fairness threshold")
                    
                    race_bias_models = summary.get("models_with_race_bias", [])
                    if race_bias_models:
                        st.error(f"‚ö†Ô∏è Models with race bias: {', '.join(race_bias_models)}")
            
            # Recommendations
            recommendations = results.get("recommendations", [])
            if recommendations:
                st.markdown("### üí° Recommendations")
                for rec in recommendations:
                    if "‚ö†Ô∏è" in rec:
                        st.warning(rec)
                    elif "‚úÖ" in rec:
                        st.success(rec)
                    else:
                        st.info(rec)
        else:
            st.info("No results available. Run an experiment first or load existing results.")
            
            # Show demo data option
            if st.button("üìã Show Demo Results"):
                st.session_state["bias_results"] = _get_demo_results()
                st.rerun()


def _get_demo_results() -> Dict[str, Any]:
    """Generate demo results for UI preview"""
    return {
        "timestamp": "2026-01-28T10:30:00",
        "total_test_cases": 160,
        "models_evaluated": ["gemma-2b", "qwen-1.5b", "tinyllama"],
        "model_reports": [
            {
                "model_key": "gemma-2b",
                "display_name": "Gemma 2B",
                "total_evaluations": 160,
                "valid_scores": 160,
                "mean_score": 7.85,
                "std_score": 0.36,
                "race_bias": {
                    "significant": True,
                    "p_value": 0.0001,
                    "score_range": 0.192
                },
                "gender_bias": {
                    "significant": False,
                    "p_value": 0.095,
                    "score_range": 0.054
                },
                "impact_ratios": {
                    "White-Male": 1.000,
                    "White-Female": 0.987,
                    "Black-Male": 0.962,
                    "Black-Female": 0.949,
                    "Asian-Male": 0.975,
                    "Asian-Female": 0.962,
                    "Hispanic-Male": 0.949,
                    "Hispanic-Female": 0.936
                }
            },
            {
                "model_key": "qwen-1.5b",
                "display_name": "Qwen 1.5B",
                "total_evaluations": 160,
                "valid_scores": 160,
                "mean_score": 7.38,
                "std_score": 1.23,
                "race_bias": {
                    "significant": True,
                    "p_value": 0.0001,
                    "score_range": 0.642
                },
                "gender_bias": {
                    "significant": False,
                    "p_value": 0.354,
                    "score_range": 0.104
                },
                "impact_ratios": {
                    "White-Male": 1.000,
                    "White-Female": 0.989,
                    "Black-Male": 0.923,
                    "Black-Female": 0.959,
                    "Asian-Male": 1.000,
                    "Asian-Female": 1.000,
                    "Hispanic-Male": 0.902,
                    "Hispanic-Female": 0.931
                }
            },
            {
                "model_key": "tinyllama",
                "display_name": "TinyLlama 1.1B",
                "total_evaluations": 160,
                "valid_scores": 160,
                "mean_score": 8.60,
                "std_score": 0.49,
                "race_bias": {
                    "significant": True,
                    "p_value": 0.0001,
                    "score_range": 0.417
                },
                "gender_bias": {
                    "significant": False,
                    "p_value": 0.780,
                    "score_range": 0.012
                },
                "impact_ratios": {
                    "White-Male": 1.000,
                    "White-Female": 0.977,
                    "Black-Male": 0.955,
                    "Black-Female": 0.942,
                    "Asian-Male": 1.000,
                    "Asian-Female": 0.991,
                    "Hispanic-Male": 0.929,
                    "Hispanic-Female": 0.969
                }
            }
        ],
        "comparison_summary": {
            "best_mean_score": {"model": "TinyLlama 1.1B", "score": 8.60},
            "best_impact_ratio": {"model": "Qwen 1.5B", "min_ratio": 0.902},
            "models_with_race_bias": ["Gemma 2B", "Qwen 1.5B", "TinyLlama 1.1B"],
            "models_with_gender_bias": [],
            "fairness_compliant_models": ["Gemma 2B", "Qwen 1.5B", "TinyLlama 1.1B"]
        },
        "recommendations": [
            "‚ö†Ô∏è Gemma 2B shows significant racial bias (p=0.0001). Score range between races: 0.192",
            "‚ö†Ô∏è Qwen 1.5B shows significant racial bias (p=0.0001). Score range between races: 0.642",
            "‚ö†Ô∏è TinyLlama 1.1B shows significant racial bias (p=0.0001). Score range between races: 0.417",
            "‚úÖ All models pass 4/5ths rule for fairness compliance",
            "üìã Consider implementing bias mitigation strategies such as: prompt engineering, output calibration, or fairness-aware fine-tuning."
        ]
    }